{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project A15\n",
    "\n",
    "### Inputs\n",
    "###### Source: Smartphone 3\n",
    "###### Signal: accelerometer and gyroscope\n",
    "###### Validation: 2, 5 and 10 fold subject-wise\n",
    "\n",
    "### Result Presentation\n",
    "Compare error based on number of folds in cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT: This code is only for single sample code trials. It will be iterated over all the samples at the later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os\n",
    "from pandas import read_csv\n",
    "import scipy.signal as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting motion sequence\n",
    "\n",
    "def extract_seq(time, half_width, avg_freq_a):\n",
    "    time_max = time[int(np.shape(time)[0])-1]\n",
    "    time_mean = time[int(np.shape(time)[0])-1]/2\n",
    "    time_max = time_mean + half_width\n",
    "    time_min = time_mean - half_width\n",
    "    pos = np.array([0,0])\n",
    "    pos[0] = int(avg_freq_a*time_min)\n",
    "    pos[1] = int(avg_freq_a*time_max)\n",
    "    return pos #returns the indices of the starting point and end point of the extracted sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(inputs):    \n",
    "    \n",
    "    inputs = inputs.lower()\n",
    "    \n",
    "    file_split = ((inputs).replace(\"_\",\" \")).split() #split the file names for identification\n",
    "    \n",
    "    subject = ''.join(filter(lambda j: j.isdigit(), file_split[0])) #extract subject number from name\n",
    "    \n",
    "    sample = ''.join(filter(lambda j: j.isdigit(), file_split[1]))\n",
    "    \n",
    "    #gait\n",
    "    if ((inputs.find('nor') or inputs.find('rma') or inputs.find('mal')) != -1):\n",
    "        gait = 0\n",
    "        \n",
    "    elif ((inputs.find('imp') or inputs.find('pai') or inputs.find('red')) != -1):\n",
    "        gait = 1\n",
    "        \n",
    "    else:\n",
    "        gait = -1 #returns -1 to detect erronous file names\n",
    "    \n",
    "    metadata = [subject, gait] #returns an array of subject number and gait (normal = 0, impaired = 1)\n",
    "    \n",
    "    #print(file_split)\n",
    "    #print(subject)\n",
    "    #print(gait)\n",
    "    \n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_db(files):\n",
    "\n",
    "    # The function extracts and resamples the imported data to match the database format\n",
    "    \n",
    "    sensor = ['\\\\Accelerometer.csv','\\\\Gyroscope.csv']\n",
    "    \n",
    "    # Read CSV into idividual arrays for accelerometer and gyroscope\n",
    "    \n",
    "    path_main_acc = \"E:\\MME_CAME\\STUDY\\CIE\\PROJECT A\\Smartphone3\\\\\" + files + sensor[0]\n",
    "    df_acc = read_csv(path_main_acc)\n",
    "    path_main_gyro = \"E:\\MME_CAME\\STUDY\\CIE\\PROJECT A\\Smartphone3\\\\\" + files + sensor[1]\n",
    "    df_gyro = read_csv(path_main_gyro)\n",
    "    \n",
    "    # Calculate start time and end time for extracting data\n",
    "    \n",
    "    acc_time = df_acc[df_acc.columns[0]]\n",
    "    gyro_time = df_gyro[df_gyro.columns[0]]\n",
    "    \n",
    "    acc_time_max  = acc_time[int(np.shape(acc_time)[0])-1]\n",
    "    acc_time_mean = acc_time[int(np.shape(acc_time)[0])-1]/2\n",
    "    acc_time_max = acc_time_mean + 10\n",
    "    acc_time_min = acc_time_mean - 10\n",
    "    \n",
    "    acc_min  = np.where(round(acc_time,1)  == round(acc_time_min,1))\n",
    "    acc_max  = np.where(round(acc_time,1)  == round(acc_time_max,1))\n",
    "    gyro_min = np.where(round(gyro_time,1) == round(acc_time_min,1))\n",
    "    gyro_max = np.where(round(gyro_time,1) == round(acc_time_max,1))\n",
    "    \n",
    "    acc_min  = int(np.mean(acc_min))\n",
    "    acc_max  = int(np.mean(acc_max))\n",
    "    gyro_min = int(np.mean(gyro_min))\n",
    "    gyro_max = int(np.mean(gyro_max))\n",
    "    \n",
    "    \n",
    "    freq = 500 #resampling freqency\n",
    "    time_window = 20 #Sample time window\n",
    "    df_samples = time_window*freq\n",
    "    df = np.zeros((6,df_samples))\n",
    "    \n",
    "    for i in range(6):\n",
    "        \n",
    "        if (i<3):\n",
    "            data = np.array(df_acc[df_acc.columns[i+1]])\n",
    "            data = data[acc_min:acc_max]\n",
    "            data = ss.resample(data, df_samples)\n",
    "        \n",
    "        else:\n",
    "            data = np.array(df_gyro[df_gyro.columns[i-2]])\n",
    "            data = data[gyro_min:gyro_max]\n",
    "            data = ss.resample(data, df_samples)\n",
    "        \n",
    "        df[i,:] = data\n",
    "    \n",
    "    return df #Resampled data for the given files arranged in 6Xn format where rows are axes of Accelerometer and Gyroscope respectively\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  99.84 %\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "# Import folder list\n",
    "files=[]\n",
    "files = [f for f in sorted(os.listdir('Smartphone3'))]\n",
    "fol_len=int(np.shape(files)[0])\n",
    "\n",
    "main_array = []\n",
    "label_data = []\n",
    "\n",
    "# Processing data for every file\n",
    "\n",
    "for i in range(fol_len):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    df = export_db(files[i])\n",
    "    \n",
    "    #data filtering using SG filter\n",
    "    \n",
    "    for k in range(6):\n",
    "        seq = df[k,:]\n",
    "        seq = ss.savgol_filter(seq, 101 , 4)\n",
    "        df[k,:] = seq\n",
    "    \n",
    "    #identification of peaks for sengmentation\n",
    "    \n",
    "    resultant_acc = ss.savgol_filter(df[3,:], 1001 , 8)\n",
    "    peaks,_ = ss.find_peaks(resultant_acc,height=-10)\n",
    "    peaksofpeaks,_ = ss.find_peaks(resultant_acc[peaks])\n",
    "  \n",
    "    samples = 300 # define samples in each sequence\n",
    "    \n",
    "    seg_start = np.zeros(np.shape(peaks)[0])\n",
    "    seg_end = np.zeros(np.shape(peaks)[0])\n",
    "    \n",
    "    # Segmentation and resampling of data based on identified peaks\n",
    "    \n",
    "    for l in range(np.shape(peaks)[0]-1):\n",
    "        fragment = np.zeros((6, samples))\n",
    "        for k in range(6):\n",
    "            seg_start[l] = peaks[l]\n",
    "            seg_end[l] = peaks[l+1]\n",
    "            seg = (df[k,:])[int(seg_start[l]):int(seg_end[l])]\n",
    "            fragment[k,:] = ss.resample(seg, samples)\n",
    "        \n",
    "        #writing label and array\n",
    "        \n",
    "        label_data.append(label(files[i]))\n",
    "        main_array.append(fragment)\n",
    "        \n",
    "    status=str(round(i*100/fol_len,2))\n",
    "    print('Status: ',status,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100' '101' '102' '103' '104' '105' '106' '107' '108' '109' '111' '115'\n",
      " '116' '117' '118' '119' '125' '126' '127' '128' '129' '130' '131' '132'\n",
      " '133' '134' '135' '136' '137' '138' '139' '25' '27' '29' '30' '31' '32'\n",
      " '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46' '47'\n",
      " '48' '49' '50' '53' '54' '55' '56' '57' '58' '59' '60' '61' '62' '63'\n",
      " '64' '65' '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76' '77'\n",
      " '78' '79' '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91'\n",
      " '92' '93' '94' '95' '96' '97' '98' '99']\n"
     ]
    }
   ],
   "source": [
    "# prepare labels for splitting train and test data\n",
    "\n",
    "label_data = np.array(label_data)\n",
    "main_array = np.array(main_array)\n",
    "unique_labels = np.unique((label_data)[:,0])\n",
    "print(unique_labels)\n",
    "xxx = int(np.shape(unique_labels)[0])\n",
    "count = []\n",
    "for i in range(xxx):\n",
    "    count.append(np.where(label_data == (np.unique((label_data)[:,0])[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test data as per folds\n",
    "\n",
    "X_test = []\n",
    "X_train = []\n",
    "Y_test = []\n",
    "Y_train = []\n",
    "\n",
    "folds = 10 #define number of folds\n",
    "fraction = 1/folds\n",
    "\n",
    "for i in range(0,int(xxx*fraction)):\n",
    "    indices_sub = np.array(np.where(label_data[:,0] == unique_labels[i]))\n",
    "    #print(np.shape(indices_sub)[1])\n",
    "    for j in range(np.shape(indices_sub)[1]):\n",
    "        X_test.append(main_array[indices_sub[:,j],:].reshape(6,300))\n",
    "        Y_test.append(label_data[indices_sub[:,j],1])\n",
    "        \n",
    "for i in range(int(xxx*0.2),xxx):\n",
    "    indices_sub = np.array(np.where(label_data[:,0] == unique_labels[i]))\n",
    "    #print(np.shape(indices_sub)[1])\n",
    "    for j in range(np.shape(indices_sub)[1]):\n",
    "        X_train.append(main_array[indices_sub[:,j],:].reshape(6,300))\n",
    "        Y_train.append(label_data[indices_sub[:,j],1])\n",
    "        \n",
    "Y_train = np.transpose(Y_train)[0].astype(int)\n",
    "Y_test = np.transpose(Y_test)[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18977, 6, 300)\n",
      "(2113, 6, 300)\n",
      "(18977,)\n",
      "(2113,)\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data splitting\n",
    "\n",
    "folds_check = int((np.shape(Y_train)[0]+np.shape(Y_test)[0])/np.shape(Y_test)[0])\n",
    "print(folds_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18977, 6, 300)\n",
      "(2113, 6, 300)\n",
      "(18977,)\n",
      "(2113,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = np.array(X_train).astype(float)\n",
    "X_test = np.array(X_test).astype(float)\n",
    "Y_train = np.array(Y_train).astype(float)\n",
    "Y_test = np.array(Y_test).astype(float)\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(main_array, np.transpose(label_data)[1], test_size=0.1)\n",
    "\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model with tensorflow keras \n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(6,300)),\n",
    "    tf.keras.layers.Dense(128,activation ='sigmoid'),\n",
    "    tf.keras.layers.Dense(64 ,activation ='sigmoid'),\n",
    "    tf.keras.layers.Dense(32 ,activation ='sigmoid'),\n",
    "    tf.keras.layers.Dense(16 ,activation ='sigmoid'),\n",
    "    tf.keras.layers.Dense(2  , activation = 'softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# compile the model, set optimizer, loss and metric functions\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'mean_squared_error',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# train the neural network for 10 training epochs\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds = 10\n",
      "Loss = 0.25\n",
      "accuracy = 0.4723142385482788\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0);\n",
    "\n",
    "print('Folds =', folds)\n",
    "print('Loss =', loss)\n",
    "print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictions for the test set\n",
    "predictions = model.predict(X_test[:5])\n",
    "\n",
    "print(np.round(predictions[4],6))\n",
    "print(predictions.shape)\n",
    "# show the image\n",
    "for i in range(5):\n",
    "    #plt.plot(X_test[i])\n",
    "    # Get the label with the highest prediction value\n",
    "    print('Label = ', np.argmax(predictions[i]))\n",
    "    #plt.show()\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folds = 2\n",
    "Loss = 0.25002381205558777\n",
    "accuracy = 0.5051564574241638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folds = 5\n",
    "Loss = 0.25\n",
    "accuracy = 0.5037577152252197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folds = 10\n",
    "Loss = 0.25\n",
    "accuracy = 0.4723142385482788"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
